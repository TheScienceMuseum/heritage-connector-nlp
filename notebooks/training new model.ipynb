{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training a new model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "from hc_nlp.pipeline import ThesaurusMatcher, EntityFilter, MapEntityTypes, DateMatcher\n",
    "from hc_nlp.model_testing import test_ner\n",
    "from hc_nlp.io import load_text_and_annotations_from_labelstudio\n",
    "from hc_nlp.spacy_helpers import correct_entity_boundaries\n",
    "from hc_nlp import constants\n",
    "\n",
    "import pprint\n",
    "pp = pprint.PrettyPrinter(indent=2)\n",
    "\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "import random\n",
    "import warnings\n",
    "\n",
    "import spacy\n",
    "from spacy.util import minibatch, compounding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_lg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Create training set\n",
    "This is all the data that isn't in the test set at `../data/TEST_SET_2020-12-10-12-43-04.zip`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = load_text_and_annotations_from_labelstudio('../data/TEST_SET_2020-12-10-12-43-04.zip', spacy_model=nlp)\n",
    "test_text = [i[0] for i in test_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(('Soup plate, white ceramic, Newhaven to Dieppe service logo at top. Made by C Mc D Mann & Co Ltd, Hanley. Overall: 44 mm x 253 mm, 0.73kg.',\n",
       "  [(97, 103, 'LOC'), (27, 35, 'LOC'), (39, 45, 'LOC'), (75, 95, 'ORG')]),\n",
       " 'Soup plate, white ceramic, Newhaven to Dieppe service logo at top. Made by C Mc D Mann & Co Ltd, Hanley. Overall: 44 mm x 253 mm, 0.73kg.')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data[0], test_text[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = pd.read_json(\"../data/text_all.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(291620, 264098, 2297)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# text_test may contain duplicates as the descriptions come from labelled data, but text_train will not\n",
    "text_train = text[~text['text'].isin(test_text)].drop_duplicates(subset='text')\n",
    "text_test = text[text['text'].isin(test_text)]\n",
    "\n",
    "len(text), len(text_train), len(text_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = None\n",
    "\n",
    "text_train = text_train.sample(train_size, random_state=42) if train_size is not None else text_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Train new model\n",
    "\n",
    "### 2.1 Initialise model with rule- and thesaurus-based matching\n",
    "\n",
    "- `nlp` is our out-of-the-box model\n",
    "- `nlp_thes` is our model with the additional components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-01-05 16:32:17,575 - hc_nlp.pipeline - INFO - Loading thesaurus from ../data/labels_all_unambiguous_types_people_orgs.jsonl\n",
      "2021-01-05 16:32:22,993 - hc_nlp.pipeline - INFO - 17016 term thesaurus imported in 5s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(['tagger', 'parser', 'ner'],\n",
       " ['tagger',\n",
       "  'parser',\n",
       "  'DateMatcher',\n",
       "  'ner',\n",
       "  'ThesaurusMatcher',\n",
       "  'EntityFilter'])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model with rules for dates before & thesaurus after, with overwrite\n",
    "datematcher = DateMatcher(nlp)\n",
    "thes_ow = ThesaurusMatcher(nlp, thesaurus_path=\"../data/labels_all_unambiguous_types_people_orgs.jsonl\", \n",
    "                                  case_sensitive=False, overwrite_ents=True)\n",
    "entityfilter = EntityFilter(ent_labels_ignore=['DATE'])\n",
    "mapentitytypes = MapEntityTypes(nlp, validate_mapping=False)\n",
    "\n",
    "nlp_thes = spacy.load(\"en_core_web_lg\")\n",
    "nlp_thes.add_pipe(datematcher, before='ner')\n",
    "nlp_thes.add_pipe(thes_ow, after='ner')\n",
    "nlp_thes.add_pipe(entityfilter, last=True)\n",
    "# nlp_thes.add_pipe(mapentitytypes)\n",
    "\n",
    "# nlp.add_pipe(mapentitytypes)\n",
    "\n",
    "nlp.pipe_names, nlp_thes.pipe_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Create training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(7, 17, 'PERSON')]"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_entity_list(doc, correct=True):\n",
    "    entity_list = []\n",
    "    \n",
    "    for ent in doc.ents:\n",
    "        start = doc[ent.start].idx\n",
    "\n",
    "        # TODO: correct end in hc-nlp\n",
    "        end = doc[ent.end-1].idx + len(doc[ent.end-1].text)\n",
    "        entity_list.append((start, end, ent.label_))\n",
    "        \n",
    "    if correct:\n",
    "        entity_list_old = entity_list\n",
    "        entity_list = correct_entity_boundaries(nlp, doc.text, entity_list)\n",
    "\n",
    "        if entity_list_old != entity_list:\n",
    "            print(entity_list_old, entity_list, doc.text)\n",
    "\n",
    "    return entity_list\n",
    "\n",
    "text = \"Who is Shaka Khan?\"\n",
    "doc = nlp_thes(text)\n",
    "\n",
    "get_entity_list(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000 records created in 47 seconds\n"
     ]
    }
   ],
   "source": [
    "TRAIN_DATA = []\n",
    "model = nlp_thes\n",
    "\n",
    "start = time.time()\n",
    "for doc in model.pipe(text_train['text'].head(5000).tolist()):\n",
    "    TRAIN_DATA.append(\n",
    "        (doc.text, {\"entities\": get_entity_list(doc)})\n",
    "    )\n",
    "end = time.time()\n",
    "\n",
    "print(f\"{len(TRAIN_DATA)} records created in {int(end-start)} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Train new model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp_new = spacy.load('en_core_web_lg')\n",
    "ner = nlp_new.get_pipe(\"ner\")\n",
    "\n",
    "n_iter = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Losses {'ner': 8752.958746125605}\n",
      "Losses {'ner': 6644.080891124536}\n",
      "Losses {'ner': 6149.880774482395}\n",
      "Losses {'ner': 5765.229031543167}\n",
      "Losses {'ner': 5513.222365782627}\n",
      "Losses {'ner': 5328.265107249336}\n",
      "Losses {'ner': 5129.718474322531}\n",
      "Losses {'ner': 4897.516061056674}\n",
      "Losses {'ner': 4790.09224472084}\n",
      "Losses {'ner': 4546.9993544531935}\n",
      "Losses {'ner': 4522.889306091676}\n",
      "Losses {'ner': 4303.650312348633}\n",
      "Losses {'ner': 4120.754619620491}\n",
      "Losses {'ner': 4185.125311100743}\n",
      "Losses {'ner': 4085.752658769397}\n",
      "Losses {'ner': 3987.9173164355343}\n",
      "Losses {'ner': 3840.4816759482833}\n",
      "Losses {'ner': 3851.970716302068}\n",
      "Losses {'ner': 3708.6101216559177}\n",
      "Losses {'ner': 3638.098496863698}\n",
      "Losses {'ner': 3540.1594012227843}\n",
      "Losses {'ner': 3516.0517860667323}\n",
      "Losses {'ner': 3416.2412796115245}\n",
      "Losses {'ner': 3299.033486725564}\n",
      "Losses {'ner': 3297.038785775817}\n",
      "Losses {'ner': 3308.0953211830397}\n",
      "Losses {'ner': 3206.700624895452}\n",
      "Losses {'ner': 3206.3090628694317}\n",
      "Losses {'ner': 3019.968656119732}\n",
      "Losses {'ner': 3033.4680495122057}\n",
      "Losses {'ner': 3131.812609569239}\n",
      "Losses {'ner': 3083.2983942382007}\n",
      "Losses {'ner': 2971.3915206977817}\n",
      "Losses {'ner': 2916.423398686416}\n",
      "Losses {'ner': 2956.217797843607}\n",
      "Losses {'ner': 2742.0621770834073}\n",
      "Losses {'ner': 2970.768965261748}\n",
      "Losses {'ner': 2891.7119488046533}\n",
      "Losses {'ner': 2807.5992867915033}\n",
      "Losses {'ner': 2682.283066863121}\n",
      "Losses {'ner': 2739.5632490544363}\n",
      "Losses {'ner': 2705.555600106333}\n",
      "Losses {'ner': 2763.8496402075225}\n",
      "Losses {'ner': 2605.193544568898}\n",
      "Losses {'ner': 2638.0592739970975}\n",
      "Losses {'ner': 2570.143544913353}\n",
      "Losses {'ner': 2449.2524788362766}\n",
      "Losses {'ner': 2616.6849580259814}\n",
      "Losses {'ner': 2543.4716075769857}\n",
      "Losses {'ner': 2428.3555186286944}\n",
      "Losses {'ner': 2438.703389937139}\n",
      "Losses {'ner': 2408.2027270148446}\n",
      "Losses {'ner': 2343.9697961208167}\n",
      "Losses {'ner': 2537.3679662103896}\n",
      "Losses {'ner': 2424.615057307285}\n",
      "Losses {'ner': 2323.6967341962354}\n",
      "Losses {'ner': 2418.8591555435078}\n",
      "Losses {'ner': 2419.4998091437005}\n",
      "Losses {'ner': 2235.226297270359}\n",
      "Losses {'ner': 2294.3192254029054}\n",
      "Losses {'ner': 2274.1997683155746}\n",
      "Losses {'ner': 2251.681552589697}\n",
      "Losses {'ner': 2373.55603798051}\n",
      "Losses {'ner': 2355.9535229573653}\n",
      "Losses {'ner': 2308.287719486374}\n",
      "Losses {'ner': 2210.932201108775}\n",
      "Losses {'ner': 2216.55052443968}\n",
      "Losses {'ner': 2170.9811668729735}\n",
      "Losses {'ner': 2140.7705069186636}\n",
      "Losses {'ner': 2192.8599387246168}\n",
      "Losses {'ner': 2181.3782540622947}\n",
      "Losses {'ner': 2117.8894046403207}\n",
      "Losses {'ner': 2110.0379984242595}\n",
      "Losses {'ner': 2093.863497038598}\n",
      "Losses {'ner': 2175.0918572088694}\n",
      "Losses {'ner': 2048.3786171284073}\n",
      "Losses {'ner': 2029.4593612564508}\n",
      "Losses {'ner': 2102.3992461353173}\n",
      "Losses {'ner': 2048.74647338641}\n",
      "Losses {'ner': 2071.637172906073}\n",
      "Losses {'ner': 1997.245463836712}\n",
      "Losses {'ner': 2049.2165154026857}\n",
      "Losses {'ner': 2057.325966700174}\n",
      "Losses {'ner': 1940.0044774321843}\n",
      "Losses {'ner': 2025.7272203888062}\n",
      "Losses {'ner': 1952.9769266335315}\n",
      "Losses {'ner': 2014.0121730771875}\n",
      "Losses {'ner': 1982.5640277517575}\n",
      "Losses {'ner': 1907.403431147106}\n",
      "Losses {'ner': 2013.9375599293012}\n",
      "Losses {'ner': 1921.4182820704746}\n",
      "Losses {'ner': 1909.1537077308449}\n",
      "Losses {'ner': 1851.4087154332801}\n",
      "Losses {'ner': 1944.1623176152964}\n",
      "Losses {'ner': 1841.4665932370913}\n",
      "Losses {'ner': 1937.0940005430405}\n",
      "Losses {'ner': 1935.8041882877417}\n",
      "Losses {'ner': 1979.9884111102544}\n",
      "Losses {'ner': 1872.7659428302927}\n",
      "Losses {'ner': 1934.756588629847}\n"
     ]
    }
   ],
   "source": [
    "required_pipes = ['ner']\n",
    "disable_pipes = [pipe for pipe in nlp_new.pipe_names if pipe not in required_pipes]\n",
    "\n",
    "with nlp_new.disable_pipes(*disable_pipes), warnings.catch_warnings():\n",
    "    # show warnings for misaligned entity spans once\n",
    "    warnings.filterwarnings(\"once\", category=UserWarning, module='spacy')\n",
    "    \n",
    "    for itn in range(n_iter):\n",
    "        random.shuffle(TRAIN_DATA)\n",
    "        losses = {}\n",
    "        \n",
    "        batches = minibatch(TRAIN_DATA, size=compounding(4.0, 32.0, 1.001))\n",
    "        for batch in batches:\n",
    "            texts, annotations = zip(*batch)\n",
    "            try:\n",
    "                nlp_new.update(\n",
    "                    texts,\n",
    "                    annotations,\n",
    "                    drop=0.5, # dropout for regularization\n",
    "                    losses=losses,\n",
    "                )\n",
    "            # TODO: drop this except and let spaCy handle the errors\n",
    "            except:\n",
    "                print(batch)\n",
    "                print()\n",
    "                break\n",
    "\n",
    "        print(\"Losses\", losses)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
